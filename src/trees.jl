function grow_tree(node::TreeNode,
    X_bin::AbstractMatrix{UInt8},
    bags, edges,
    Î´::AbstractArray{T,1}, Î´Â²::AbstractArray{T,1}, ğ‘¤::AbstractArray{T,1},
    splits::Vector{SplitInfo{T, Int}},
    tracks::Vector{SplitTrack{T}},
    params::EvoTreeRegressor,
    ğ‘–::I, ğ‘—::J) where {R<:Real, T<:AbstractFloat, I<:BitSet, J<:AbstractArray{Int, 1}, S<:Int}

    if node.depth < params.max_depth && node.âˆ‘ğ‘¤ >= params.min_weight
        # Search best split for each feature - to be multi-threaded

        # initializde node splits info and tracks - colsample size (ğ‘—)
        # splits = Vector{SplitInfo{Float64, Int64}}(undef, length(splits))
        @threads for feat in 1:length(splits)
            # splits[feat].gain = -Inf
            # splits[feat].âˆ‘Î´L = 0.0
            # splits[feat].âˆ‘Î´Â²L = 0.0
            # splits[feat].âˆ‘ğ‘¤L = 0.0
            # splits[feat].âˆ‘Î´R = 0.0
            # splits[feat].âˆ‘Î´Â²R = 0.0
            # splits[feat].âˆ‘ğ‘¤R = 0.0
            # splits[feat].gainL = -Inf
            # splits[feat].gainR = -Inf
            # splits[feat].ğ‘– = 0
            # splits[feat].feat = feat
            # splits[feat].cond = 0.0
            splits[feat] = SplitInfo{Float64, Int64}(-Inf, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -Inf, -Inf, 0, feat, 0.0)
        end
        # tracks = Vector{SplitTrack{Float64}}(undef, length(tracks))
        # @threads for feat in ğ‘—
        #     tracks[feat] = SplitTrack{Float64}(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -Inf, -Inf, -Inf)
        # end

        @threads for feat in ğ‘—
            find_split_turbo!(bags[feat], view(X_bin,:,feat), Î´, Î´Â², ğ‘¤, node.âˆ‘Î´::T, node.âˆ‘Î´Â²::T, node.âˆ‘ğ‘¤::T, params, splits[feat], tracks[feat], edges[feat], ğ‘–)
            splits[feat].feat = feat
        end
        # find best split
        best = get_max_gain(splits)
        # grow node if best split improve gain
        if best.gain > node.gain + params.Î³
            node = SplitNode(
            grow_tree(LeafNode(node.depth + 1, best.âˆ‘Î´L, best.âˆ‘Î´Â²L, best.âˆ‘ğ‘¤L, best.gainL, 0.0), X_bin, bags, edges, Î´, Î´Â², ğ‘¤, splits, tracks, params, intersect(ğ‘–, union(bags[best.feat][1:best.ğ‘–]...)), ğ‘—),
            grow_tree(LeafNode(node.depth + 1, best.âˆ‘Î´R, best.âˆ‘Î´Â²R, best.âˆ‘ğ‘¤R, best.gainR, 0.0), X_bin, bags, edges, Î´, Î´Â², ğ‘¤, splits, tracks, params, intersect(ğ‘–, union(bags[best.feat][(best.ğ‘–+1):end]...)), ğ‘—),
            best.feat,
            best.cond)
        end
    end
    # if isa(node, LeafNode) node.pred = - node.âˆ‘Î´ / (node.âˆ‘Î´Â² + params.Î») end
    if isa(node, LeafNode) node.pred = pred_leaf(params.loss, node, params, Î´Â²) end
    return node
end


# extract the gain value from the vector of best splits and return the split info associated with best split
function get_max_gain(splits::Vector{SplitInfo{Float64,Int}})
    gains = (x -> x.gain).(splits)
    feat = findmax(gains)[2]
    best = splits[feat]
    # best.feat = feat
    return best
end


# function grow_gbtree(X::AbstractArray, Y, params::EvoTreeRegressor)
#
#     Î¼ = mean(Y)
#     pred = zeros(size(Y,1)) .* 0
#     Î´, Î´Â² = grad_hess(pred, Y)
#     âˆ‘Î´, âˆ‘Î´Â² = sum(Î´), sum(Î´Â²)
#     gain = get_gain(âˆ‘Î´, âˆ‘Î´Â², params.Î»)
#
#     bias = LeafNode(1, 0.0, 0.0, gain, 0.0)
#     model = GBTree([bias], params)
#
#     for i in 1:params.nrounds
#         # select random rows and cols
#         #X, Y = X[row_ids, col_ids], Y[row_ids]
#         # get gradients
#         Î´, Î´Â² = grad_hess(pred, Y)
#         âˆ‘Î´, âˆ‘Î´Â² = sum(Î´), sum(Î´Â²)
#         gain = get_gain(âˆ‘Î´, âˆ‘Î´Â², params.Î»)
#         # assign a root and grow tree
#         root = TreeLeaf(1, âˆ‘Î´, âˆ‘Î´Â², gain, 0.0)
#         # grow tree
#         tree = grow_tree(root, view(X, :, :), view(Î´, :), view(Î´Â², :), params)
#         # get update predictions
#         pred += predict(tree, X) .* params.Î·
#         # update push tree to model
#         push!(model.trees, tree)
#
#         println("iter: ", i, " completed")
#     end
#     return model
# end

# grow_gbtree
function grow_gbtree(X::AbstractArray{R, 2}, Y::AbstractArray{T, 1}, params::EvoTreeRegressor;
    X_eval::AbstractArray{R, 2} = Array{R, 2}(undef, (0,0)), Y_eval::AbstractArray{T, 1} = Array{Float64, 1}(undef, 0),
    early_stopping_rounds=Int(1e5), print_every_n=100, verbosity=1) where {R<:Real, T<:AbstractFloat}

    seed!(params.seed)

    Î¼ = mean(Y)
    if typeof(params.loss) == Logistic
        Î¼ = logit(Î¼)
    elseif typeof(params.loss) == Poisson
        Î¼ = log(Î¼)
    end
    pred = ones(size(Y, 1)) .* Î¼

    # initialize gradients and weights
    Î´, Î´Â² = zeros(Float64, size(Y, 1)), zeros(Float64, size(Y, 1))
    ğ‘¤ = ones(Float64, size(Y, 1))

    # eval init
    if size(Y_eval, 1) > 0
        pred_eval = ones(size(Y_eval, 1)) .* Î¼
    end

    bias = LeafNode(1, 0.0, 0.0, 0.0, 0.0, Î¼)
    gbtree = GBTree([bias], params, Metric())

    X_size = size(X)
    ğ‘–_ = collect(1:X_size[1])
    ğ‘—_ = collect(1:X_size[2])

    edges = get_edges(X, params.nbins)
    X_bin = binarize(X, edges)
    bags = Vector{Vector{BitSet}}(undef, size(ğ‘—_, 1))
    @threads for feat in 1:size(ğ‘—_, 1)
        bags[feat] = find_bags(X_bin[:,feat])
    end

    # initialize metric
    if params.metric != :none
        metric_track = Metric()
        metric_best = Metric()
        iter_since_best = 0
    end

    # loop over nrounds
    for i in 1:params.nrounds
        # select random rows and cols
        ğ‘– = ğ‘–_[sample(ğ‘–_, ceil(Int, params.rowsample * X_size[1]), replace = false)]
        ğ‘— = ğ‘—_[sample(ğ‘—_, ceil(Int, params.colsample * X_size[2]), replace = false)]

        # get gradients
        update_grads!(params.loss, params.Î±, pred, Y, Î´, Î´Â², ğ‘¤)
        âˆ‘Î´, âˆ‘Î´Â², âˆ‘ğ‘¤ = sum(Î´[ğ‘–]), sum(Î´Â²[ğ‘–]), sum(ğ‘¤[ğ‘–])
        gain = get_gain(params.loss, âˆ‘Î´, âˆ‘Î´Â², âˆ‘ğ‘¤, params.Î»)

        # initializde node splits info and tracks - colsample size (ğ‘—)
        splits = Vector{SplitInfo{Float64, Int64}}(undef, X_size[2])
        for feat in ğ‘—_
            splits[feat] = SplitInfo{Float64, Int64}(-Inf, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -Inf, -Inf, 0, feat, 0.0)
        end
        tracks = Vector{SplitTrack{Float64}}(undef, X_size[2])
        for feat in ğ‘—_
            tracks[feat] = SplitTrack{Float64}(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -Inf, -Inf, -Inf)
        end

        # assign a root and grow tree
        root = LeafNode(1, âˆ‘Î´, âˆ‘Î´Â², âˆ‘ğ‘¤, gain, 0.0)
        tree = grow_tree(root, X_bin, bags, edges, Î´, Î´Â², ğ‘¤, splits, tracks, params, BitSet(ğ‘–), ğ‘—)
        # update push tree to model
        push!(gbtree.trees, tree)

        # get update predictions
        predict!(pred, tree, X)
        # eval predictions
        if size(Y_eval, 1) > 0
            predict!(pred_eval, tree, X_eval)
        end

        # callback function
        if params.metric != :none

            if size(Y_eval, 1) > 0
                metric_track.metric .= eval_metric(Val{params.metric}(), pred_eval, Y_eval, params.Î±)
            else
                metric_track.metric .= eval_metric(Val{params.metric}(), pred, Y, params.Î±)
            end

            if metric_track.metric < metric_best.metric
                metric_best.metric .=  metric_track.metric
                metric_best.iter .=  i
            else
                iter_since_best += 1
            end

            if mod(i, print_every_n) == 0 && verbosity > 0
                display(string("iter:", i, ", eval: ", metric_track.metric))
            end
            iter_since_best >= early_stopping_rounds ? break : nothing
        end
    end #end of nrounds

    if params.metric != :none
        gbtree.metric.iter .= metric_best.iter
        gbtree.metric.metric .= metric_best.metric
    end
    return gbtree
end

# # grow_gbtree - continue training
# function grow_gbtree!(model::GBTree, X::AbstractArray{R, 2}, Y::AbstractArray{T, 1};
#     X_eval::AbstractArray{R, 2} = Array{R, 2}(undef, (0,0)), Y_eval::AbstractArray{T, 1} = Array{Float64, 1}(undef, 0),
#     early_stopping_rounds=Int(1e5), print_every_n=100, verbosity=1) where {R<:Real, T<:AbstractFloat}
#
#     params = model.params
#     seed!(params.seed)
#
#     # initialize gradients and weights
#     Î´, Î´Â² = zeros(Float64, size(Y, 1)), zeros(Float64, size(Y, 1))
#     ğ‘¤ = ones(Float64, size(Y, 1))
#
#     pred = predict(model, X)
#     # eval init
#     if size(Y_eval, 1) > 0
#         pred_eval = predict(model, X_eval)
#     end
#
#     X_size = size(X)
#     ğ‘–_ = collect(1:X_size[1])
#     ğ‘—_ = collect(1:X_size[2])
#
#     edges = get_edges(X, params.nbins)
#     X_bin = binarize(X, edges)
#     bags = Vector{Vector{BitSet}}(undef, size(ğ‘—_, 1))
#     @threads for feat in 1:size(ğ‘—_, 1)
#         bags[feat] = find_bags(X_bin[:,feat])
#     end
#
#     # initialize train nodes
#     train_nodes = Vector{TrainNode{Float64, BitSet, Array{Int64, 1}, Int64}}(undef, 2^params.max_depth-1)
#     for feat in 1:2^params.max_depth-1
#         train_nodes[feat] = TrainNode(0, -Inf, -Inf, -Inf, -Inf, BitSet([0]), [0])
#     end
#
#     # initialize metric
#     if params.metric != :none
#         metric_track = model.metric
#         metric_best = model.metric
#         iter_since_best = 0
#     end
#
#     # loop over nrounds
#     for i in 1:params.nrounds
#         # select random rows and cols
#         ğ‘– = ğ‘–_[sample(ğ‘–_, ceil(Int, params.rowsample * X_size[1]), replace = false)]
#         ğ‘— = ğ‘—_[sample(ğ‘—_, ceil(Int, params.colsample * X_size[2]), replace = false)]
#
#         # get gradients
#         update_grads!(params.loss, params.Î±, pred, Y, Î´, Î´Â², ğ‘¤)
#         âˆ‘Î´, âˆ‘Î´Â², âˆ‘ğ‘¤ = sum(Î´[ğ‘–]), sum(Î´Â²[ğ‘–]), sum(ğ‘¤[ğ‘–])
#         gain = get_gain(params.loss, âˆ‘Î´, âˆ‘Î´Â², âˆ‘ğ‘¤, params.Î»)
#
#         # initializde node splits info and tracks - colsample size (ğ‘—)
#         splits = Vector{SplitInfo{Float64, Int64}}(undef, X_size[2])
#         for feat in ğ‘—_
#             splits[feat] = SplitInfo{Float64, Int64}(-Inf, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -Inf, -Inf, 0, feat, 0.0)
#         end
#         tracks = Vector{SplitTrack{Float64}}(undef, X_size[2])
#         for feat in ğ‘—_
#             tracks[feat] = SplitTrack{Float64}(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -Inf, -Inf, -Inf)
#         end
#
#         # assign a root and grow tree
#         train_nodes[1] = TrainNode(1, âˆ‘Î´, âˆ‘Î´Â², âˆ‘ğ‘¤, gain, BitSet(ğ‘–), ğ‘—)
#         tree = grow_tree(bags, Î´, Î´Â², ğ‘¤, params, train_nodes, splits, tracks, edges, X_bin)
#         # update push tree to model
#         push!(model.trees, tree)
#
#         # get update predictions
#         predict!(pred, tree, X)
#         # eval predictions
#         if size(Y_eval, 1) > 0
#             predict!(pred_eval, tree, X_eval)
#         end
#
#         # callback function
#         if params.metric != :none
#
#             if size(Y_eval, 1) > 0
#                 metric_track.metric .= eval_metric(Val{params.metric}(), pred_eval, Y_eval, params.Î±)
#             else
#                 metric_track.metric .= eval_metric(Val{params.metric}(), pred, Y, params.Î±)
#             end
#
#             if metric_track.metric < metric_best.metric
#                 metric_best.metric .=  metric_track.metric
#                 metric_best.iter .=  i
#             else
#                 iter_since_best += 1
#             end
#
#             if mod(i, print_every_n) == 0 && verbosity > 0
#                 display(string("iter:", i, ", eval: ", metric_track.metric))
#             end
#             iter_since_best >= early_stopping_rounds ? break : nothing
#         end
#     end #end of nrounds
#
#     if params.metric != :none
#         model.metric.iter .= metric_best.iter
#         model.metric.metric .= metric_best.metric
#     end
#     return model
# end
